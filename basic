一：---------requests库
几个主要方法：
# 构造一个请求 支撑以下方法的基础方法
requests.requests（method, url, **kwargs）

# 获取HTTP网页的主要方法 get
requests.get(url, param=None, **kwargs)
'''
r = requests.get(url)   
返回一个response对象
r的属性：
r.status_code  请求返回状态200表成功 404失败
r.text  响应内容的字符串形式 对应url的页面内容
r.encoding  header中猜测的编码方式
r.apparent_encoding  从内容中分析出的编码方式
r.content  二进制形式
'''

# 
requests.head(url, **kwargs)

# 向HTML网页提交post请求
requests.post(url, data=None, json=None, **kwargs)

#           提交put请求
requests.put(url, data=None,**kwargs)

#           提交局部修改请求 对应HTTP的patch
requests.patch(url, data=None, **kwargs)

#           删除
requests.delete(url, **kwargs)


'''
异常：
.ConnectionError   网路连接错误异常，如DNS查询失败、拒绝连接等
.HTTPError     HTTP错误异常
.URLRquired    URL缺失异常
.TooManyRedirects    超过最大重定向次数
.ConnectTimeout    连接远程服务器超时异常
.Timeout      请求URL超时
r.raise_for_status()   不是200时产生requests.HTTPError
'''

##### 通用框架 ####
import requests

def getHTMLText(url):
  try: # 网络连接有风险，异常处理很重要
    r = requests.get(url, timeout=30)
    r.raise_for_status() #如果状态不是200，引发HTTPError异常
    r.encoding = r.apparent_encoding
    return r.text
  except:
    return "产生异常"
    
if __name__ == "__main__":
url = "http://www.baidu.com"
print(getHTMLText(url))


########################################################################################
# -------------beautifulsoup库
'''
beautifulsoup 对应一个HTML/XML文档的全部内容。
库解析器：
bs4----HTML解析器   BeautifulSoup(mk, 'html.parser')
lxml-----HTML                    (mk, 'lxml')
lxml----XML                      (mk, 'xml')
html5lib----                     (mk, 'html5lib')

类基本元素：
Tag---标签，最基本的信息组织单元，用<> </>标明开头和结尾
Name---标签名。<p>....</p>的名字是'p'，格式：<tag>.name
Attributes---标签的属性，字典形式，格式：<tag>.attrs
NavigableString---标签内非属性字符串，<>...</>中字符串，格式：<tag>.string
Comment---标签字符串的注释部分，一种特殊的Comment类型


HTML遍历方法：
标签树的下行遍历：
.contents----子节点的列表，将<tag>所有子节点存入列表
.children----子节点迭代类型，与contents类似
.descendants---子孙节点的迭代类型，包含所有子孙节点，用于循环遍历
for child in soup.body.children:
    print(child)   # 遍历子节点
for child in soup.body.descendants:
    print(child)  # 遍历子孙节点
    
标签树的上行遍历：
.parent---节点的父标签
.parents--节点先辈标签的迭代类型，循环遍历先辈节点
soup = BeautifulSoup(demom, "html.parser")
for parent in soup.a.parents:   # 遍历soup的a标签的先辈标签
    if parent is None:
        print(parent)
    else:
        print(parent.name)
        
标签树的平行遍历：
.next_sibling---返回HTML文本顺序的下一个平行节点标签
.previous_sibling---返回按照HTML文本顺序的上一个平行节点标签
.next_siblings---迭代类型
.previous_siblings---迭代类型
for sibling in soup.a.next_siblings:  # 遍历后续节点
    print(sibling)
for siblingin soup.a.previous_siblings:  # 遍历前序节点
    print(sibling)
'''


'''
信息标记的三种形式：XML， JSON， YAML
XML                     JSON                                  YMAL
<name>...</name>    "key": "value"                         key: value key: #Comment
<name />            "key": ["value1", "value2"]            key: ‐value1
<!‐‐ ‐‐>             "key": {"subkey", "subvalue"}         subkey: subvalue ‐value2

信息提取的一般方法：
      法一：完整解析信息的标记形式，再提取关键信息。
需要标记解析器 例如：bs4库的标签树遍历
优点：信息解析准确
缺点：提取过程繁琐，速度慢
      法二：无视标记形式，直接搜索关键信息，对信息的文本查找函数即可
优点：提取过程简洁，速度较快
缺点：提取结果准确性与信息内容相关
      融合方法：结合形势解析与搜索方法，提取关键信息需要标记解析器及文本查找函数
'''


#######实例------提取HTML中所有URL链接------######
# 思路----搜索所有a标签，解析<a>标签格式，提取href后的链接内容
import requests
from bs4 import BeautifulSoup


r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo, "html.parser")
for link in soup.find_all("a"):
    print(link.get("href"))

'''
<>.find_all(name, attrs, recursive, string, **kwargs)
name---对标签名称的检索字符串，如soup.find_all(['a', 'b'])
attrs---对标签属性值的检索字符串，可标注属性检索，如soup.find_all('p', 'course')、soup.find_all(id="link1")
recursive---是否对子孙全部检索，默认True。如soup.find_all('a', recursive=False)
string---<>.....</>中字符串区域的检索字符串，soup.find_all(string = re.compile("python"))
<tag>(...) 等价于<tag>.find_all()            soup()等价于soup.find_all()

<>.find()----搜索且只返回一个结果，字符串类型，同 .find_all()参数
<>.find_parents()----在先辈节点中搜索，返回列表类型，同 .find_all()参数
<>.find_parent()----在先辈节点中返回一个结果，字符串类型，同 .find_all()参数
<>.find_next_siblings()----在后续平行节点中搜索，返回列表类型，同 .find_all()参数
<>.find_next_sibling()----在后续平行节点中返回一个结果，字符串类型，同 .find_all()参数
<>.find_previous_siblings()----在前序平行节点中搜索，返回列表类型，同 .find_all()参数
<>.find_previous_sibling()----在前序平行节点中返回一个结果，字符串类型，同 .find_all()参数
'''

























