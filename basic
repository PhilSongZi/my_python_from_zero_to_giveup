一：---------requests库
几个主要方法：
# 构造一个请求 支撑以下方法的基础方法
requests.requests（method, url, **kwargs）

# 获取HTTP网页的主要方法 get
requests.get(url, param=None, **kwargs)
'''
r = requests.get(url)   
返回一个response对象
r的属性：
r.status_code  请求返回状态200表成功 404失败
r.text  响应内容的字符串形式 对应url的页面内容
r.encoding  header中猜测的编码方式
r.apparent_encoding  从内容中分析出的编码方式
r.content  二进制形式
'''

# 
requests.head(url, **kwargs)

# 向HTML网页提交post请求
requests.post(url, data=None, json=None, **kwargs)

#           提交put请求
requests.put(url, data=None,**kwargs)

#           提交局部修改请求 对应HTTP的patch
requests.patch(url, data=None, **kwargs)

#           删除
requests.delete(url, **kwargs)


'''
异常：
.ConnectionError   网路连接错误异常，如DNS查询失败、拒绝连接等
.HTTPError     HTTP错误异常
.URLRquired    URL缺失异常
.TooManyRedirects    超过最大重定向次数
.ConnectTimeout    连接远程服务器超时异常
.Timeout      请求URL超时
r.raise_for_status()   不是200时产生requests.HTTPError
'''

##### 通用框架 ####
import requests

def getHTMLText(url):
  try: # 网络连接有风险，异常处理很重要
    r = requests.get(url, timeout=30)
    r.raise_for_status() #如果状态不是200，引发HTTPError异常
    r.encoding = r.apparent_encoding
    return r.text
  except:
    return "产生异常"
    
if __name__ == "__main__":
url = "http://www.baidu.com"
print(getHTMLText(url))


########################################################################################
# -------------beautifulsoup库
'''
beautifulsoup 对应一个HTML/XML文档的全部内容。
库解析器：
bs4----HTML解析器   BeautifulSoup(mk, 'html.parser')
lxml-----HTML                    (mk, 'lxml')
lxml----XML                      (mk, 'xml')
html5lib----                     (mk, 'html5lib')

类基本元素：
Tag---标签，最基本的信息组织单元，用<> </>标明开头和结尾
Name---标签名。<p>....</p>的名字是'p'，格式：<tag>.name
Attributes---标签的属性，字典形式，格式：<tag>.attrs
NavigableString---标签内非属性字符串，<>...</>中字符串，格式：<tag>.string
Comment---标签字符串的注释部分，一种特殊的Comment类型


HTML遍历方法：
标签树的下行遍历：
.contents----子节点的列表，将<tag>所有子节点存入列表
.children----子节点迭代类型，与contents类似
.descendants---子孙节点的迭代类型，包含所有子孙节点，用于循环遍历
for child in soup.body.children:
    print(child)   # 遍历子节点
for child in soup.body.descendants:
    print(child)  # 遍历子孙节点
    
标签树的上行遍历：
.parent---节点的父标签
.parents--节点先辈标签的迭代类型，循环遍历先辈节点
soup = BeautifulSoup(demom, "html.parser")
for parent in soup.a.parents:   # 遍历soup的a标签的先辈标签
    if parent is None:
        print(parent)
    else:
        print(parent.name)
        
标签树的平行遍历：
.next_sibling---返回HTML文本顺序的下一个平行节点标签
.previous_sibling---返回按照HTML文本顺序的上一个平行节点标签
.next_siblings---迭代类型
.previous_siblings---迭代类型
for sibling in soup.a.next_siblings:  # 遍历后续节点
    print(sibling)
for siblingin soup.a.previous_siblings:  # 遍历前序节点
    print(sibling)
'''


'''
信息标记的三种形式：XML， JSON， YAML
XML                     JSON                                  YMAL
<name>...</name>    "key": "value"                         key: value key: #Comment
<name />            "key": ["value1", "value2"]            key: ‐value1
<!‐‐ ‐‐>             "key": {"subkey", "subvalue"}         subkey: subvalue ‐value2

信息提取的一般方法：
      法一：完整解析信息的标记形式，再提取关键信息。
需要标记解析器 例如：bs4库的标签树遍历
优点：信息解析准确
缺点：提取过程繁琐，速度慢
      法二：无视标记形式，直接搜索关键信息，对信息的文本查找函数即可
优点：提取过程简洁，速度较快
缺点：提取结果准确性与信息内容相关
      融合方法：结合形势解析与搜索方法，提取关键信息需要标记解析器及文本查找函数
'''


#######实例------提取HTML中所有URL链接------######
# 思路----搜索所有a标签，解析<a>标签格式，提取href后的链接内容
import requests
from bs4 import BeautifulSoup


r = requests.get("https://python123.io/ws/demo.html")
demo = r.text
soup = BeautifulSoup(demo, "html.parser")
for link in soup.find_all("a"):
    print(link.get("href"))

'''
<>.find_all(name, attrs, recursive, string, **kwargs)
name---对标签名称的检索字符串，如soup.find_all(['a', 'b'])
attrs---对标签属性值的检索字符串，可标注属性检索，如soup.find_all('p', 'course')、soup.find_all(id="link1")
recursive---是否对子孙全部检索，默认True。如soup.find_all('a', recursive=False)
string---<>.....</>中字符串区域的检索字符串，soup.find_all(string = re.compile("python"))
<tag>(...) 等价于<tag>.find_all()            soup()等价于soup.find_all()

<>.find()----搜索且只返回一个结果，字符串类型，同 .find_all()参数
<>.find_parents()----在先辈节点中搜索，返回列表类型，同 .find_all()参数
<>.find_parent()----在先辈节点中返回一个结果，字符串类型，同 .find_all()参数
<>.find_next_siblings()----在后续平行节点中搜索，返回列表类型，同 .find_all()参数
<>.find_next_sibling()----在后续平行节点中返回一个结果，字符串类型，同 .find_all()参数
<>.find_previous_siblings()----在前序平行节点中搜索，返回列表类型，同 .find_all()参数
<>.find_previous_sibling()----在前序平行节点中返回一个结果，字符串类型，同 .find_all()参数
'''


#######正则表达式#############################
'''
.        表示任何单个字符
[ ]      字符集，对单个字符给出取值范围       [abc]表示a、b、c，[az]表示a到z单个字符
[^ ]     非字符集，对单个字符给出排除范围     [^abc]表示非a或b或c的单个字符
*        前一个字符0次或无限次扩展            abc* 表示 ab、abc、abcc、abccc等
+        前一个字符1次或无限次扩展            abc+ 表示 abc、abcc、abccc等
?        前一个字符0次或1次扩展               abc？表示 ab、abc
|        左右表达式任意一个                   abc|def 表示 abc、def
{m}      扩展前一个字符m次                    ab{2}c 表示 abbc
{m,n}    扩展前一个字符m至n次（含n）          ab{1,2}c 表示 abc、abbc
^        匹配字符串开头^abc                   表示abc且在一个字符串的开头
$        匹配字符串结尾abc$                   表示abc且在一个字符串的结尾
( )      分组标记，内部只能使用 | 操作符      (abc) 表示abc，(abc|def) 表示 abc、def
\d       数字，等价于[09]
\w       单词字符，等价于[A-Za-z0-9]

经典re表达式
^[A-Za-z]+$       26个字母组成的字符串
^[A-Za-z0-9]+$        26个字母和数字组成的字符串
^-?\d+$         整数形式的字符串
^[0-9]*[1-9][0-9]*$       正整数形式的字符串
[1-9]\d{5}          中国境内邮政编码，6位
[\u4e00-\u9fa5]         匹配中文字符
\d{3}-\d{8}|\d{4}-\d{7}         国内电话号码，如010-68913536

'''

'''
re库：raw string原生字符串（不包含转义符的字符串）------eg:r"text"
re.search()       在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象
re.match()        从一个字符串的开始位置起匹配正则表达式，返回match对象
re.findall()      搜索字符串，以列表类型返回全部能匹配的子串

re.split(pattern, string, maxsplit=0, flags=0)        
将一个字符串按照正则表达式匹配结果进行分割。返回列表类型
maxsplit:最大分割数，剩余部分作为最后一个元素输出。

re.finditer()     搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象

re.sub(pattern, repl, string, count=0, flags=0)          
在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串.
repl：替换匹配字符串的字符串.
count：匹配的最大替换次数.

re.search(pattern, string,flags=0)   同   match,findall
----pattern：正则表达式的字符串或原生字符串表示
----string：待匹配字符串
----flags：正则表达式使用时的控制标记


re库的match对象：
属性：
.string     待匹配的文本
.re         匹配时使用的pattern对象（正则表达式）
.pos        正则表达式搜索文本的开始位置
.endpos     正则表达式搜索文本的结束位置
方法：
.group(0)   获得一次匹配后的字符串
.start()    匹配字符串在原始字符串的开始位置
.end()      匹配字符串在原始字符串的结束位置
.span()     返回(.start(), .end())
'''




















